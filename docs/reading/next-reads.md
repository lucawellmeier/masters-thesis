---
layout: page
title: Updated list of potential next reads

---

Added after the second meeting (11/05/2022)
- [Rudi, Rosasco 2017] Generalization properties of learning with random features
- [Rudi, Camoriano, Rosasco 2015] Less is more: Nyström computational regularization
- [Rudi, Carratino, Rosasco 2017] FALKON: An optimal large-scale kernel method
- [Meanti, Carratino, Rosasco, Rudi 2020] Kernel methods through the roof: handling billions of points efficiently
- [Liu, Huang, Chen, Suykens 2020] Random Features for Kernel Approximation: A survey on algorithms, theory and beyond
- [Belkin 2021] Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation
- [Bartlett, Montanari, Rakhlin 2021] Deep learning: a statistical viewpoint

Added in the time after the first meeting:
- [Hastie, Montanari, Rosset, Tibshirani 2019] Surprises in High-Dimensional Ridgeless Least Squares Interpolation
- [Belkin 2018] Approximation beats Concentration? An approximation view on inference with smooth radial kernels
- [Belkin, Rakhlin, Tsybakov 2018] Does data interpolation contradict statistical optimality?
- [Jacot, Gabriel, Hongler 2018] Neural Tangent Kernel: Convergence and Generalization in Neural Networks
- [Du, Zhai, Poczós, Singh 2019] Gradient Descent Provably Optimizes Over-parametrized Neural Networks
- [Du, Lee, Li, Wang, Zhai 2019] Gradient Descent Finds Global Minima of Deep Neural Networks
- [Allen-Zhu, Li, Song 2019] A Convergence Theory for Deep Learning via Over-Parametrization
- [Chizat, Bach 2018] A Note on Lazy Training in Supervised Differentiable Programming
- [Cao, Chen, Belkin, Gu 2022] Benign Overfitting in Two-layer Convolutional Neural Networks
- [Adlam, Pennington 2020] The neural tangent kernel in high-dimensions: Triple descent and a multi-scale theory of generalization
- [Li, Zhou, Getton 2021] Towards an understanding of benign overfitting in neural networks
- [Wu, Xu 2020] On the optimal weighted $\ell_2$ regularization in overparametrized linear regression
- [Chatterji, Long 2020] Finite-sample analysis of interpolating linear classifiers in the overparametrized regime
- [Zou, Wu, Braverman, Gu, Kakade 2021] Benign overfitting of constant-stepsize sgd for linear regression
- [Cao, Gu, Belkin 2021] Risk bounds for over-parametrized maximum margin classification on sub-gaussian mixtures
- [Montanari, Zhong 2020] The interpolation phase transition in neural networks: Memorization and generalization under lazy training
- [Hofmann, Schölkopf, Smola 2008] Kernel Methods in Machine Learning
- [Belkin, Hsu, Mitra 2018] Overftting or perfect fitting? Risk bounds for classification and regression rules that interpolate
- [2020. Hastie, Montanari, Rosset, Tibshirani] Surprises in High-Dimensional Ridgeless Least Squares Interpolation
- [2018. Belkin, Hsu, Ma, Mandal] Reconciling modern machine learning practice and the bias-variance trade-off
- [2020. Chinot, Löffler, van de Geer] On the robustness of minimum norm interpolators and regularized empirical risk minimizers
- [2020. Shah, Basu, Kyrillidis Sanghavi] On Generalization of Adaptive Methods for Over-parametrized Linear Regression
- [2021. Caron, Chrétien] Benign Overfitting of fully connected Deep Nets: A Sobolev Space viewpoint
