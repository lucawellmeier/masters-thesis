---
layout: page
title: Updated list of potential next reads

---

- [Hastie, Montanari, Rosset, Tibshirani 2019] Surprises in High-Dimensional Ridgeless Least Squares Interpolation
- [Belkin 2018] Approximation beats Concentration? An approximation view on inference with smooth radial kernels
- [Belkin, Rakhlin, Tsybakov 2018] Does data interpolation contradict statistical optimality?
- [Belkin, Hsu, Xu 2019] Two models of double descent for weak features
- [Jacot, Gabriel, Hongler 2018] Neural Tangent Kernel: Convergence and Generalization in Neural Networks
- [Du, Zhai, Poczós, Singh 2019] Gradient Descent Provably Optimizes Over-parametrized Neural Networks
- [Du, Lee, Li, Wang, Zhai 2019] Gradient Descent Finds Global Minima of Deep Neural Networks
- [Allen-Zhu, Li, Song 2019] A Convergence Theory for Deep Learning via Over-Parametrization
- [Chizat, Bach 2018] A Note on Lazy Training in Supervised Differentiable Programming
- [Cao, Chen, Belkin, Gu 2022] Benign Overfitting in Two-layer Convolutional Neural Networks
- [Adlam, Pennington 2020] The neural tangent kernel in high-dimensions: Triple descent and a multi-scale theory of generalization
- [Li, Zhou, Getton 2021] Towards an understanding of benign overfitting in neural networks
- [Wu, Xu 2020] On the optimal weighted $\ell_2$ regularization in overparametrized linear regression
- [Chatterji, Long 2020] Finite-sample analysis of interpolating linear classifiers in the overparametrized regime
- [Zou, Wu, Braverman, Gu, Kakade 2021] Benign overfitting of constant-stepsize sgd for linear regression
- [Cao, Gu, Belkin 2021] Risk bounds for over-parametrized maximum margin classification on sub-gaussian mixtures
- [Montanari, Zhong 2020] The interpolation phase transition in neural networks: Memorization and generalization under lazy training
- [Hofmann, Schölkopf, Smola 2008] Kernel Methods in Machine Learning
- [Belkin, Hsu, Mitra 2018] Overftting or perfect fitting? Risk bounds for classification and regression rules that interpolate
- [2020. Hastie, Montanari, Rosset, Tibshirani] Surprises in High-Dimensional Ridgeless Least Squares Interpolation
- [2018. Belkin, Hsu, Ma, Mandal] Reconciling modern machine learning practice and the bias-variance trade-off
- [2020. Chinot, Löffler, van de Geer] On the robustness of minimum norm interpolators and regularized empirical risk minimizers
- [2020. Shah, Basu, Kyrillidis Sanghavi] On Generalization of Adaptive Methods for Over-parametrized Linear Regression
- [2021. Caron, Chrétien] Benign Overfitting of fully connected Deep Nets: A Sobolev Space viewpoint
